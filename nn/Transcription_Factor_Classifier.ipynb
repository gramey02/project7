{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4d2d52-e5e5-4a88-806f-d113628085b5",
   "metadata": {},
   "source": [
    "# Transcription Factor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b054ee10-df30-48b7-a4d1-82b8e1b22236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import NeuralNetwork\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from TFC_io import read_text_file, read_fasta_file\n",
    "from preprocess import sample_seqs, one_hot_encode_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1fc43-7596-4d78-bbb0-44027f8a6673",
   "metadata": {},
   "source": [
    "### Read in Rap1 motif examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc241a21-3b83-48bc-bbb0-cd7438b0d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rap1_motifs = read_text_file(\"../data/rap1-lieb-positives.txt\")\n",
    "labels = [True for i in range(len(rap1_motifs))] #generate labels, where True = positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503309a4-3098-4d45-8b27-9c4f428d8894",
   "metadata": {},
   "source": [
    "### Read in negative example from yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45616f58-f609-4158-9176-fec59944bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_neg = read_fasta_file(\"../data/yeast-upstream-1k-negative.fa\")\n",
    "labels = labels + [False for i in range(len(yeast_neg))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aea433-a9c4-43e2-9ba6-2a65bb21d282",
   "metadata": {},
   "source": [
    "### Explain Sampling Scheme\n",
    "I chose to use oversampling to correct the class imbalance in the data by resampling the positive class with replacement until it had the same number of observations as the negative class. While this can make the classification model prone to ovefitting, it prevents information loss and avoids an extremely small sample size if we were to downsample the yeast data to the size of the rap1_motifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c939f-3bfd-46bf-8388-7344c03d6366",
   "metadata": {},
   "source": [
    "### Implement sampling scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3869fdb-f20e-44a9-9d19-c10aa8df2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = rap1_motifs + yeast_neg #combine the two lists of seqs\n",
    "new_seqs, new_labels = sample_seqs(seqs, labels) #fix class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9310c5-b0b6-406e-b878-39b86f2fa900",
   "metadata": {},
   "source": [
    "### Generate training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8d5c66-2ce4-4cf9-822b-286724f7014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a 70/30 train/test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(new_seqs, new_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79975b2-609a-4978-a697-215574221662",
   "metadata": {},
   "source": [
    "### One hot encode the sequences in the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be96314e-f2d8-4e19-972b-2f21bd9dbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = one_hot_encode_seqs(X_train)\n",
    "X_val_encoded = one_hot_encode_seqs(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef360f5-9ca3-429e-8a03-642105cfa7f4",
   "metadata": {},
   "source": [
    "### Define plotting function so loss history of neural network can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5e86ff-e089-410f-aaee-587fccf904cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(per_epoch_loss_train, per_epoch_loss_val):\n",
    "    \"\"\"\n",
    "    Plots the loss history after training is complete.\n",
    "    \"\"\"\n",
    "    loss_hist = per_epoch_loss_train\n",
    "    loss_hist_val = per_epoch_loss_val\n",
    "    assert len(loss_hist) > 0, \"Need to run training before plotting loss history\"\n",
    "    fig, axs = plt.subplots(2, figsize=(8,8))\n",
    "    fig.suptitle('Loss History')\n",
    "    axs[0].plot(np.arange(len(loss_hist)), loss_hist)\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[1].plot(np.arange(len(loss_hist_val)), loss_hist_val)\n",
    "    axs[1].set_title('Validation Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    axs[0].set_ylabel('Train Loss')\n",
    "    axs[1].set_ylabel('Val Loss')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596489c5-3ee4-4ea8-8318-b05fcf101c1a",
   "metadata": {},
   "source": [
    "### Train a neural network\n",
    "I will be exploring some hyperparameters in the next few code chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df066119-6905-4477-88c4-87255a6362d7",
   "metadata": {},
   "source": [
    "Number of hidden layers = 1, hidden layer activation function = relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "928c5bc8-96fc-4b9d-a8e4-2979d14b4a75",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,1) and (15,1) not aligned: 1 (dim 1) != 15 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e48c6d9b6c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#fit the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/UCSF/Algorithms/project7/nn/nn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0;31m#forward pass through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                 \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                 \u001b[0;31m#calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss_func\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"mean squared error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/UCSF/Algorithms/project7/nn/nn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mb_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#get current bias terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mA_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_activation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_curr\u001b[0m \u001b[0;31m#store current A matrix in cache dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/UCSF/Algorithms/project7/nn/nn.py\u001b[0m in \u001b[0;36m_single_forward\u001b[0;34m(self, W_curr, b_curr, A_prev, activation)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m#based on how they are initialized, the weights are already in their de facto \"transposed form\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (5,1) and (15,1) not aligned: 1 (dim 1) != 15 (dim 0)"
     ]
    }
   ],
   "source": [
    "#start with a neural network with only 1 hidden layer with 5 nodes and go from there (final output should stem from a sigmoid function)\n",
    "nn_arch = [{'input_dim': 1, 'output_dim': 5, 'activation': 'relu'}, \n",
    "           {'input_dim': 5, 'output_dim': 1, 'activation': 'sigmoid'}]\n",
    "classifier = NeuralNetwork(nn_arch, \n",
    "                          lr=0.1, \n",
    "                          seed=42, \n",
    "                          batch_size=15, \n",
    "                          epochs=100, \n",
    "                          loss_function = \"binary cross entropy\")\n",
    "#make sure to expand the dimensions of the input data and convert both input and output data to arrays\n",
    "X_train_encoded = np.array(np.expand_dims(X_train_encoded, axis = 1))\n",
    "X_val_encoded = np.array(np.expand_dims(X_val_encoded, axis = 1))\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "#fit the neural network\n",
    "classifier.fit(X_train_encoded, y_train, X_val_encoded, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995251bf-f7ac-4f45-b3c6-5b5b19a7d3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
