{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4d2d52-e5e5-4a88-806f-d113628085b5",
   "metadata": {},
   "source": [
    "# Transcription Factor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b054ee10-df30-48b7-a4d1-82b8e1b22236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import NeuralNetwork\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from TFC_io import read_text_file, read_fasta_file\n",
    "from preprocess import sample_seqs, one_hot_encode_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1fc43-7596-4d78-bbb0-44027f8a6673",
   "metadata": {},
   "source": [
    "### Read in Rap1 motif examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc241a21-3b83-48bc-bbb0-cd7438b0d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rap1_motifs = read_text_file(\"../data/rap1-lieb-positives.txt\")\n",
    "labels = [True for i in range(len(rap1_motifs))] #generate labels, where True = positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503309a4-3098-4d45-8b27-9c4f428d8894",
   "metadata": {},
   "source": [
    "### Read in negative example from yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45616f58-f609-4158-9176-fec59944bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_neg = read_fasta_file(\"../data/yeast-upstream-1k-negative.fa\")\n",
    "labels = labels + [False for i in range(len(yeast_neg))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aea433-a9c4-43e2-9ba6-2a65bb21d282",
   "metadata": {},
   "source": [
    "### Explain Sampling Scheme\n",
    "I chose to use oversampling to correct the class imbalance in the data by resampling the positive class with replacement until it had the same number of observations as the negative class. While this can make the classification model prone to ovefitting, it prevents information loss and avoids an extremely small sample size if we were to downsample the yeast data to the size of the rap1_motifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c939f-3bfd-46bf-8388-7344c03d6366",
   "metadata": {},
   "source": [
    "### Implement sampling scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3869fdb-f20e-44a9-9d19-c10aa8df2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = rap1_motifs + yeast_neg #combine the two lists of seqs\n",
    "new_seqs, new_labels = sample_seqs(seqs, labels) #fix class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9310c5-b0b6-406e-b878-39b86f2fa900",
   "metadata": {},
   "source": [
    "### Generate training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8d5c66-2ce4-4cf9-822b-286724f7014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a 70/30 train/test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(new_seqs, new_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79975b2-609a-4978-a697-215574221662",
   "metadata": {},
   "source": [
    "### One hot encode the sequences in the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be96314e-f2d8-4e19-972b-2f21bd9dbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = one_hot_encode_seqs(X_train)\n",
    "X_val_encoded = one_hot_encode_seqs(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef360f5-9ca3-429e-8a03-642105cfa7f4",
   "metadata": {},
   "source": [
    "### Define plotting function so loss history of neural network can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e86ff-e089-410f-aaee-587fccf904cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(self):\n",
    "    \"\"\"\n",
    "    Plots the loss history after training is complete.\n",
    "    \"\"\"\n",
    "    loss_hist = self.loss_history_train\n",
    "    loss_hist_val = self.loss_history_val\n",
    "    assert len(loss_hist) > 0, \"Need to run training before plotting loss history\"\n",
    "    fig, axs = plt.subplots(2, figsize=(8,8))\n",
    "    fig.suptitle('Loss History')\n",
    "    axs[0].plot(np.arange(len(loss_hist)), loss_hist)\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[1].plot(np.arange(len(loss_hist_val)), loss_hist_val)\n",
    "    axs[1].set_title('Validation Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    axs[0].set_ylabel('Train Loss')\n",
    "    axs[1].set_ylabel('Val Loss')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596489c5-3ee4-4ea8-8318-b05fcf101c1a",
   "metadata": {},
   "source": [
    "### Train a neural network\n",
    "I will be exploring some hyperparameters in the next few code chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df066119-6905-4477-88c4-87255a6362d7",
   "metadata": {},
   "source": [
    "Number of hidden layers = 1, hidden layer activation function = relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c5bc8-96fc-4b9d-a8e4-2979d14b4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with a neural network with only 1 hidden layer with 5 nodes and go from there (final output should stem from a sigmoid function)\n",
    "nn_arch = [{'input_dim': 1, 'output_dim': 5, 'activation': 'relu'}, \n",
    "           {'input_dim': 5, 'output_dim': 1, 'activation:': 'sigmoid'}]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
