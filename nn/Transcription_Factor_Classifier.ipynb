{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4d2d52-e5e5-4a88-806f-d113628085b5",
   "metadata": {},
   "source": [
    "# Transcription Factor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b054ee10-df30-48b7-a4d1-82b8e1b22236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import NeuralNetwork\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from TFC_io import read_text_file, read_fasta_file\n",
    "from preprocess import sample_seqs, one_hot_encode_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1fc43-7596-4d78-bbb0-44027f8a6673",
   "metadata": {},
   "source": [
    "### Read in Rap1 motif examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc241a21-3b83-48bc-bbb0-cd7438b0d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rap1_motifs = read_text_file(\"../data/rap1-lieb-positives.txt\")\n",
    "labels = [True for i in range(len(rap1_motifs))] #generate labels, where True = positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503309a4-3098-4d45-8b27-9c4f428d8894",
   "metadata": {},
   "source": [
    "### Read in negative example from yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45616f58-f609-4158-9176-fec59944bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_neg = read_fasta_file(\"../data/yeast-upstream-1k-negative.fa\")\n",
    "labels = labels + [False for i in range(len(yeast_neg))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aea433-a9c4-43e2-9ba6-2a65bb21d282",
   "metadata": {},
   "source": [
    "### Explain Sampling Scheme\n",
    "I chose to use oversampling to correct the class imbalance in the data by resampling the positive class with replacement until it had the same number of observations as the negative class. While this can make the classification model prone to ovefitting, it prevents information loss and avoids an extremely small sample size if we were to downsample the yeast data to the size of the rap1_motifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c939f-3bfd-46bf-8388-7344c03d6366",
   "metadata": {},
   "source": [
    "### Implement sampling scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3869fdb-f20e-44a9-9d19-c10aa8df2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = rap1_motifs + yeast_neg #combine the two lists of seqs\n",
    "new_seqs, new_labels = sample_seqs(seqs, labels) #fix class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9310c5-b0b6-406e-b878-39b86f2fa900",
   "metadata": {},
   "source": [
    "### Generate training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8d5c66-2ce4-4cf9-822b-286724f7014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a 70/30 train/test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(new_seqs, new_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79975b2-609a-4978-a697-215574221662",
   "metadata": {},
   "source": [
    "### One hot encode the sequences in the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be96314e-f2d8-4e19-972b-2f21bd9dbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = one_hot_encode_seqs(X_train)\n",
    "X_val_encoded = one_hot_encode_seqs(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef360f5-9ca3-429e-8a03-642105cfa7f4",
   "metadata": {},
   "source": [
    "### Define plotting function so loss history of neural network can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5e86ff-e089-410f-aaee-587fccf904cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(per_epoch_loss_train, per_epoch_loss_val):\n",
    "    \"\"\"\n",
    "    Plots the loss history after training is complete.\n",
    "    \"\"\"\n",
    "    loss_hist = per_epoch_loss_train\n",
    "    loss_hist_val = per_epoch_loss_val\n",
    "    assert len(loss_hist) > 0, \"Need to run training before plotting loss history\"\n",
    "    fig, axs = plt.subplots(2, figsize=(8,8))\n",
    "    fig.suptitle('Loss History')\n",
    "    axs[0].plot(np.arange(len(loss_hist)), loss_hist)\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[1].plot(np.arange(len(loss_hist_val)), loss_hist_val)\n",
    "    axs[1].set_title('Validation Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    axs[0].set_ylabel('Train Loss')\n",
    "    axs[1].set_ylabel('Val Loss')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596489c5-3ee4-4ea8-8318-b05fcf101c1a",
   "metadata": {},
   "source": [
    "### Train a neural network\n",
    "I will be exploring some hyperparameters in the next few code chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df066119-6905-4477-88c4-87255a6362d7",
   "metadata": {},
   "source": [
    "Number of hidden layers = 1, hidden layer activation function = relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "928c5bc8-96fc-4b9d-a8e4-2979d14b4a75",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 15 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c88c814b84e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#fit the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/UCSF/Algorithms/project7/nn/nn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0;31m#forward pass through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                 \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                 \u001b[0;31m#calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss_func\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"mean squared error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/UCSF/Algorithms/project7/nn/nn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mb_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#get current bias terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mA_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_activation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_curr\u001b[0m \u001b[0;31m#store current A matrix in cache dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/UCSF/Algorithms/project7/nn/nn.py\u001b[0m in \u001b[0;36m_single_forward\u001b[0;34m(self, W_curr, b_curr, A_prev, activation)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m#based on how they are initialized, the weights are already in their de facto \"transposed form\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#+ b_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 15 is different from 2)"
     ]
    }
   ],
   "source": [
    "#start with a neural network with only 1 hidden layer with 5 nodes and go from there (final output should stem from a sigmoid function)\n",
    "nn_arch = [{'input_dim': 2, 'output_dim': 3, 'activation': 'relu'}, \n",
    "           {'input_dim': 3, 'output_dim': 2, 'activation': 'sigmoid'}]\n",
    "classifier = NeuralNetwork(nn_arch, \n",
    "                          lr=0.1, \n",
    "                          seed=42, \n",
    "                          batch_size=15, \n",
    "                          epochs=100, \n",
    "                          loss_function = \"binary cross entropy\")\n",
    "#make sure to expand the dimensions of the input data and convert both input and output data to arrays\n",
    "X_train_encoded = np.array(np.expand_dims(X_train_encoded, axis = 1))\n",
    "X_val_encoded = np.array(np.expand_dims(X_val_encoded, axis = 1))\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "#fit the neural network\n",
    "classifier.fit(X_train_encoded, y_train, X_val_encoded, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995251bf-f7ac-4f45-b3c6-5b5b19a7d3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.04967142, -0.01382643],\n",
       "        [ 0.06476885,  0.15230299],\n",
       "        [-0.02341534, -0.0234137 ]]),\n",
       " 'b1': array([[ 0.15792128],\n",
       "        [ 0.07674347],\n",
       "        [-0.04694744]]),\n",
       " 'W2': array([[ 0.054256  , -0.04634177, -0.04657298],\n",
       "        [ 0.02419623, -0.19132802, -0.17249178]]),\n",
       " 'b2': array([[-0.05622875],\n",
       "        [-0.10128311]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start with a neural network with only 1 hidden layer with 5 nodes and go from there (final output should stem from a sigmoid function)\n",
    "nn_arch = [{'input_dim': 2, 'output_dim': 3, 'activation': 'relu'}, \n",
    "           {'input_dim': 3, 'output_dim': 2, 'activation': 'sigmoid'}]\n",
    "classifier = NeuralNetwork(nn_arch, \n",
    "                          lr=0.1, \n",
    "                          seed=42, \n",
    "                          batch_size=15, \n",
    "                          epochs=100, \n",
    "                          loss_function = \"mean squared error\")\n",
    "#make sure to expand the dimensions of the input data and convert both input and output data to arrays\n",
    "#X_train_encoded = np.array(np.expand_dims(X_train_encoded, axis = 1))\n",
    "#X_val_encoded = np.array(np.expand_dims(X_val_encoded, axis = 1))\n",
    "#y_train = np.array(y_train)\n",
    "#y_val = np.array(y_val)\n",
    "classifier._param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311b7cc5-66d8-4c42-92ab-0a55d64e435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48322134 0.45455472]\n",
      " [0.47916976 0.43447566]\n",
      " [0.47512091 0.41460935]\n",
      " [0.47107534 0.39501693]]\n",
      "{'A1': array([[0.17993984, 0.4461183 , 0.        ],\n",
      "       [0.25162981, 0.88026198, 0.        ],\n",
      "       [0.32331978, 1.31440566, 0.        ],\n",
      "       [0.39500975, 1.74854933, 0.        ]]), 'Z1': array([[ 0.17993984,  0.4461183 , -0.11719017],\n",
      "       [ 0.25162981,  0.88026198, -0.21084823],\n",
      "       [ 0.32331978,  1.31440566, -0.3045063 ],\n",
      "       [ 0.39500975,  1.74854933, -0.39816437]]), 'A2': array([[0.48322134, 0.45455472],\n",
      "       [0.47916976, 0.43447566],\n",
      "       [0.47512091, 0.41460935],\n",
      "       [0.47107534, 0.39501693]]), 'Z2': array([[-0.06713985, -0.18228418],\n",
      "       [-0.08336922, -0.26361341],\n",
      "       [-0.0995986 , -0.34494263],\n",
      "       [-0.11582797, -0.42627186]])}\n"
     ]
    }
   ],
   "source": [
    "tempX_train = np.array([[1,2],\n",
    "                       [3,4],\n",
    "                       [5,6],\n",
    "                       [7,8],\n",
    "                       ])\n",
    "output, cache = classifier.forward(tempX_train)\n",
    "print(output)\n",
    "print(cache)\n",
    "#check this manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf89569-96d5-45a9-9381-1c9832c1fcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.732094297303906"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier._mean_squared_error(tempX_train, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe07885b-4602-46b0-bbdb-c52b88930727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25838933, -0.77272264],\n",
       "       [-1.26041512, -1.78276217],\n",
       "       [-2.26243954, -2.79269532],\n",
       "       [-3.26446233, -3.80249154]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier._mean_squared_error_backprop(tempX_train, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95aacf-7d16-4a0b-a6da-f4d44e15d6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39c5c7-1432-4537-a55e-06d308b7e3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d3258-46d3-40aa-8e59-a143c2d37114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da64a161-179c-47f9-964a-879778bb43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with a neural network with only 1 hidden layer with 5 nodes and go from there (final output should stem from a sigmoid function)\n",
    "nn_arch = [{'input_dim': 2, 'output_dim': 3, 'activation': 'relu'}, \n",
    "           {'input_dim': 3, 'output_dim': 2, 'activation': 'sigmoid'}]\n",
    "classifier = NeuralNetwork(nn_arch, \n",
    "                          lr=0.1, \n",
    "                          seed=42, \n",
    "                          batch_size=15, \n",
    "                          epochs=100, \n",
    "                          loss_function = \"mean squared error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4bcccb-8ba6-4f60-b792-070022b0adf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.04967142, -0.01382643],\n",
       "        [ 0.06476885,  0.15230299],\n",
       "        [-0.02341534, -0.0234137 ]]),\n",
       " 'b1': array([[ 0.15792128],\n",
       "        [ 0.07674347],\n",
       "        [-0.04694744]]),\n",
       " 'W2': array([[ 0.054256  , -0.04634177, -0.04657298],\n",
       "        [ 0.02419623, -0.19132802, -0.17249178]]),\n",
       " 'b2': array([[-0.05622875],\n",
       "        [-0.10128311]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier._param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fcdfa7-8cd0-448a-a95e-68d78c25c629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cfda29-ed1a-4121-b66f-38fe3977407c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff4582-a141-4a6f-8ce3-4737bb04e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, layer in enumerate(self.arch):\n",
    "    #print(layer_idx)\n",
    "    layer_idx = idx+1\n",
    "\n",
    "    curr_activation = layer['activation'] #get activation function type for the current layer\n",
    "    param_dict = self._param_dict #get current parameters\n",
    "\n",
    "    W_curr = param_dict['W' + str(layer_idx)] #get current weights (which on first pass will be randomly initialized)\n",
    "    b_curr = param_dict['b' + str(layer_idx)] #get current bias terms\n",
    "\n",
    "    A_curr,Z_curr = self._single_forward(W_curr, b_curr, A_prev, curr_activation)\n",
    "\n",
    "    cache[\"A\"+str(layer_idx)] = A_curr #store current A matrix in cache dictionary\n",
    "    cache[\"Z\"+str(layer_idx)] = Z_curr #store current Z matrix in cache dictionary\n",
    "\n",
    "\n",
    "    #update variables for next iteration\n",
    "    A_prev = A_curr\n",
    "\n",
    "output = A_curr #output should be final A values of corresponding to output layer\n",
    "\n",
    "return output,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baf50273-daf0-46b3-a83f-c12ebe40e68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0, layer: {'input_dim': 2, 'output_dim': 3, 'activation': 'relu'}\n",
      "idx: 1, layer: {'input_dim': 3, 'output_dim': 2, 'activation': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "for idx, layer in enumerate(classifier.arch):\n",
    "    print(\"idx: \" + str(idx) + \", layer: \" + str(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f873b4-cff8-4d0f-b0fb-49a9de101a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15792128],\n",
       "       [ 0.07674347],\n",
       "       [-0.04694744]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = classifier._param_dict['W1']\n",
    "b1 = classifier._param_dict['b1']\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ddedd8-2679-4678-b9c0-8d70537d5c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12690129, 0.21792622],\n",
       "       [0.32601405, 0.44107691]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_curr_nob = np.matmul(np.transpose(W1),tempX_train)\n",
    "Z_curr_nob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6539f860-33b7-4049-829e-d05b4ed716ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-18c5b7e481eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mZ_curr_nob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtempX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mZ_curr_nob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)"
     ]
    }
   ],
   "source": [
    "Z_curr_nob = np.matmul(W1,tempX_train)\n",
    "Z_curr_nob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c0782-3210-4dc7-b4bb-d3acff271c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _single_forward(\n",
    "                    W_curr: ArrayLike,\n",
    "                    b_curr: ArrayLike,\n",
    "                    A_prev: ArrayLike,\n",
    "                    activation: str) -> Tuple[ArrayLike, ArrayLike]:\n",
    "    \"\"\"\n",
    "    This method is used for a single forward pass on a single layer.\n",
    "\n",
    "    Args:\n",
    "        W_curr: ArrayLike\n",
    "            Current layer weight matrix.\n",
    "        b_curr: ArrayLike\n",
    "            Current layer bias matrix.\n",
    "        A_prev: ArrayLike\n",
    "            Previous layer activation matrix.\n",
    "        activation: str\n",
    "            Name of activation function for current layer.\n",
    "\n",
    "    Returns:\n",
    "        A_curr: ArrayLike\n",
    "            Current layer activation matrix.\n",
    "        Z_curr: ArrayLike\n",
    "            Current layer linear transformed matrix.\n",
    "    \"\"\"\n",
    "    #based on how they are initialized, the weights are already in their de facto \"transposed form\"\n",
    "    Z_curr = np.dot(np.transpose(W_curr),A_prev) + b_curr\n",
    "\n",
    "    return (Z_curr)\n",
    "\n",
    "def forward(X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "    \"\"\"\n",
    "    This method is responsible for one forward pass of the entire neural network.\n",
    "\n",
    "    Args:\n",
    "        X: ArrayLike\n",
    "            Input matrix with shape [batch_size, features].\n",
    "\n",
    "    Returns:\n",
    "        output: ArrayLike\n",
    "            Output of forward pass.\n",
    "        cache: Dict[str, ArrayLike]:\n",
    "            Dictionary storing Z and A matrices from `_single_forward` for use in backprop.\n",
    "    \"\"\"\n",
    "    cache = {} #cache to be filled later\n",
    "    A_prev = X #assign inputs to the previous A matrix\n",
    "\n",
    "    #probably want to create a for loop where you loop through each layer of the model and get Z and A matrices for each layer\n",
    "    #length of nn_arch = # of layers\n",
    "    for layer_idx in range(1,len(self.arch)+1):\n",
    "\n",
    "        curr_layer = self.arch[layer_idx] #get dictionary that represents current layer inputs and outputs\n",
    "        curr_activation = curr_layer['activation'] #get activation function type for the current layer\n",
    "        param_dict = self._param_dict #get current parameters\n",
    "\n",
    "        W_curr = param_dict['W' + str(layer_idx)] #get current weights (which on first pass will be randomly initialized)\n",
    "        b_curr = param_dict['b' + str(layer_idx)] #get current bias terms\n",
    "\n",
    "        A_curr,Z_curr = _single_forward(W_curr, b_curr, A_prev, curr_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711fe58-115e-432b-bb96-b0447dac6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
